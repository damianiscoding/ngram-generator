{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "613e9ae7-60e6-492c-8f58-959eae39551f",
      "metadata": {
        "id": "613e9ae7-60e6-492c-8f58-959eae39551f"
      },
      "source": [
        "## Install Requirements\n",
        "Run the cell, below, to install the libraries we'll be using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "337b586e-c781-49fc-a60e-bc13ce4e78e9",
      "metadata": {
        "id": "337b586e-c781-49fc-a60e-bc13ce4e78e9",
        "outputId": "5ce39115-5024-4a49-b6d5-bb79fbf787bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/414.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m409.6/414.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m409.6/414.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.3/414.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/2.5 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.4 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.16 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain langchain-core langchain_community langchain_text_splitters langgraph\n",
        "%pip install -qU langchain-google-genai\n",
        "%pip install -qU bs4\n",
        "%pip install -qU python-dotenv typing_extensions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d372d5e7-db78-4e70-abcb-527ed3172fb5",
      "metadata": {
        "id": "d372d5e7-db78-4e70-abcb-527ed3172fb5"
      },
      "source": [
        "## Load the API key into the environment\n",
        "The code, below, loads the API key and stores it where the LangChain libraries (and likely the Google libraries used by the LangChain libraries) expect to find it.\n",
        "\n",
        "**If you're running this code in Google Colab**, this code assumes you've already stored your API key as a *secret*:\n",
        "\n",
        "1. Open your Google Colab notebook and click on the 🔑 Secrets tab in the left panel.\n",
        "2. The Secrets tab is found on the left panel.\n",
        "3. Create a new secret with the name `GOOGLE_API_KEY`.\n",
        "4. Copy/paste your API key into the Value input box of `GOOGLE_API_KEY`.\n",
        "5. Toggle the button on the left to allow notebook access to the secret.\n",
        "\n",
        "Otherwise, the code assumes that you have a `.env` file that includes `GOOGLE_API_KEY=<your api key here>`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c8e14fed-2868-4890-b286-36cb11a77551",
      "metadata": {
        "id": "c8e14fed-2868-4890-b286-36cb11a77551"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "API_KEY = 'GOOGLE_API_KEY'\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import userdata\n",
        "    os.environ[API_KEY] = userdata.get(API_KEY)\n",
        "    os.environ[API_KEY]\n",
        "else:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()  # Load environment variables from .env file; should include GOOGLE_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ea1eea1-5beb-4ea6-bdb2-e58b77b43364",
      "metadata": {
        "id": "2ea1eea1-5beb-4ea6-bdb2-e58b77b43364"
      },
      "source": [
        "You can verify that your API key is where it ought to be by uncommenting and running the code cell, below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ec2eff26-1a8b-4df4-9148-faeb91c64fcb",
      "metadata": {
        "id": "ec2eff26-1a8b-4df4-9148-faeb91c64fcb",
        "outputId": "723448e3-77b8-4c69-9e7d-5fa30f23e280",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AIzaSyAFd3jwgT_lEzNImSSMf57cSUWHligw6mQ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "os.getenv(API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6be63fe-dfa6-43f6-847d-89ce013c82a6",
      "metadata": {
        "id": "d6be63fe-dfa6-43f6-847d-89ce013c82a6"
      },
      "source": [
        "## Components\n",
        "Import and instantiate a:\n",
        "  1. chat model\n",
        "  2. embedding model\n",
        "  3. in-memory vector store\n",
        "\n",
        "Note that we're using the `langchain_google_genai` library instead of the Google Vertex (or OpenAI, or Anthropic, etc.) library. That means you can't simply copy code from the LangChain tutorial. Documentation for the Google GenAI library can be found [here](https://python.langchain.com/api_reference/google_genai/index.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0a0f543d-f139-49a6-a445-a15a0d6e5f25",
      "metadata": {
        "id": "0a0f543d-f139-49a6-a445-a15a0d6e5f25"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "53cf1193-4665-48bd-83e1-c7ffa58fbd00",
      "metadata": {
        "id": "53cf1193-4665-48bd-83e1-c7ffa58fbd00"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5203071a-d2a5-4c04-b4c8-6400a1b1dc83",
      "metadata": {
        "id": "5203071a-d2a5-4c04-b4c8-6400a1b1dc83"
      },
      "outputs": [],
      "source": [
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "vector_store = InMemoryVectorStore(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "278cf55a-d311-43da-bce6-438e70b9ca26",
      "metadata": {
        "id": "278cf55a-d311-43da-bce6-438e70b9ca26"
      },
      "source": [
        "## RAG Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e2ed668-0ced-44cb-afe4-61f9e6f23d90",
      "metadata": {
        "id": "7e2ed668-0ced-44cb-afe4-61f9e6f23d90"
      },
      "source": [
        "### Scrape a Web Page\n",
        "\n",
        "We'll use the `WebBaseLoader` class to scrape a web page we'd like to ask an LLM about. It uses [Beautiful Soup](https://beautiful-soup-4.readthedocs.io/en/latest/) -- another popular library -- to parse the web page (extract its text content).\n",
        "\n",
        "Notice how, instead of writing their own HTML parser, the LangChain developers make use of another well-established library. The named parameter `bs_kwargs` is short for \"Beautiful Soup key-word arguments. We're passing to `WebBaseLoader` a set of arguments that will be passed to Beautiful Soup functions. A decision to use another library like this comes with trade-offs:\n",
        "  - To use LangChain, I don't have to write much or any code to control Beautiful Soup. LangChain handles (almost) all of it for me.\n",
        "  - But now this LangChain class is dependent on (tied to) Beautiful Soup. If Beautiful Soup changes interfaces, `WebBaseLoader` might break.\n",
        "  - And `WebBaseLoader` is also somewhat less flexible. What if Beautiful Soup isn't my prefered library or doesn't do what I need? So you'll sometimes see one library give you the ability to pass whatever HTML parser you choose. It could be Beautiful Soup or another open-source library or the HTML parser you wrote for fun.\n",
        "\n",
        "Notice also that we've decided to give Beautiful Soup some more specific instructions, taking content from HTML tags that have a class of `post-content`, `post-title`, or `post-header`. (You could navigate to the web page and open the developer tools to see just what that includes.) Doing so gives us cleaner text to use for our RAG application but at the cost of making our code less general. If I want to query a different web page, there's no reason to think it will use the same class names to identify the important bits. If we add a web page loader to KnotebookLM, we'll need to think about how best to generalize our approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b8bc34ac-f19c-40e0-a10e-f4712b208f6b",
      "metadata": {
        "id": "b8bc34ac-f19c-40e0-a10e-f4712b208f6b",
        "outputId": "80a4aec2-bb86-4cc9-a65a-0dc62654fd38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "# setting a User-Agent to avoid a Beautiful Soup warning\n",
        "# a User-Agent header tells a web server what kind of client is making the request\n",
        "os.environ['USER_AGENT'] = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        ),\n",
        "    ),\n",
        ")\n",
        "\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "443664c2-7f19-42ec-b78e-cc05d6c0005c",
      "metadata": {
        "id": "443664c2-7f19-42ec-b78e-cc05d6c0005c"
      },
      "source": [
        "`docs` is a list of `Document` objects. We only loaded one document, so the length of `docs` is 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "978e6729-d8a7-4fb1-81a7-eb2c61c211ad",
      "metadata": {
        "id": "978e6729-d8a7-4fb1-81a7-eb2c61c211ad",
        "outputId": "7a807fd2-afeb-4daa-ffd8-9da5cd45ddb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1398b08c-5ce1-41a0-8ea4-da57c5183b98",
      "metadata": {
        "id": "1398b08c-5ce1-41a0-8ea4-da57c5183b98"
      },
      "source": [
        "We can ask Python to tell us the type of that sole document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e52be27e-69bf-40b8-9950-c353492c3613",
      "metadata": {
        "id": "e52be27e-69bf-40b8-9950-c353492c3613",
        "outputId": "9083b376-5a8a-4f69-add7-7cbcedad02d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.documents.base.Document"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.documents.base.Document</b><br/>def __init__(page_content: str, **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/langchain_core/documents/base.py</a>Class for storing a piece of text and associated metadata.\n",
              "\n",
              "Example:\n",
              "\n",
              "    .. code-block:: python\n",
              "\n",
              "        from langchain_core.documents import Document\n",
              "\n",
              "        document = Document(\n",
              "            page_content=&quot;Hello, world!&quot;,\n",
              "            metadata={&quot;source&quot;: &quot;https://example.com&quot;}\n",
              "        )</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 262);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "type(docs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92fb85ca-d2d2-4ba3-937c-e755dd436bc3",
      "metadata": {
        "id": "92fb85ca-d2d2-4ba3-937c-e755dd436bc3"
      },
      "source": [
        "That's `langchain-core`'s base `Document` class. Consulting the [documentation](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) you can see it is instantiated with two notable properties: `page_content` and `metadata`. (You can also find in the documentation a link to the source code if you want to dig further.)\n",
        "\n",
        "We'll need to talk about `metadata` later. For now, let's look at the first bit of the `page_content`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1ee9db16-c120-4605-8ab0-bf9b01e3e24e",
      "metadata": {
        "id": "1ee9db16-c120-4605-8ab0-bf9b01e3e24e",
        "outputId": "ce30948c-4699-4465-e1ef-e6f613fcaefd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as i'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "docs[0].page_content[:300]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a75bc8f2-38cf-4435-bf9e-eae4aa7fcf12",
      "metadata": {
        "id": "a75bc8f2-38cf-4435-bf9e-eae4aa7fcf12"
      },
      "source": [
        "Compare it to the web page we scraped. Beautiful Soup did a pretty good job, no?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d979cb17-906f-4f88-8e6a-4a1dda941b70",
      "metadata": {
        "id": "d979cb17-906f-4f88-8e6a-4a1dda941b70"
      },
      "source": [
        "### Split the Text\n",
        "As a final pre-processing step, we'll split the text into smaller chunks. Read [why](https://python.langchain.com/docs/concepts/text_splitters/#why-split-documents).\n",
        "\n",
        "Following the tutorial, we'll use the `RecursiveCharacterTextSplitter` class. It implements a [text-structure based](https://python.langchain.com/docs/concepts/text_splitters/#text-structured-based) approach. To better understand how this splitter works and how to control it, read this [guide](https://python.langchain.com/docs/how_to/recursive_text_splitter/) and consult the [documentation](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "4ef62a9f-fd2b-4a9a-8053-3beec03d663d",
      "metadata": {
        "id": "4ef62a9f-fd2b-4a9a-8053-3beec03d663d"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=200)\n",
        "all_splits = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10418d2a-e88b-4928-b9bb-362465c61ccf",
      "metadata": {
        "id": "10418d2a-e88b-4928-b9bb-362465c61ccf"
      },
      "source": [
        "Let's see how many chunks we've split our document into."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "94ba6483-5741-455f-82e6-85702913123f",
      "metadata": {
        "id": "94ba6483-5741-455f-82e6-85702913123f",
        "outputId": "a2002c4c-0c3b-4bfd-b0de-1b7bc228f3ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "291"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "len(all_splits)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_splits[112].page_content)"
      ],
      "metadata": {
        "id": "Kexts0K_ASWa",
        "outputId": "e78b8612-33f6-4f43-9cb3-028f87a26e5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Kexts0K_ASWa",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f270c46-f04d-420e-9d4f-ca0946d902f7",
      "metadata": {
        "id": "8f270c46-f04d-420e-9d4f-ca0946d902f7"
      },
      "source": [
        "They're not all equal length. Based on the guides and documentation you've read, can you explain why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "323611eb-b7ce-4a51-86a2-12b98527d8ae",
      "metadata": {
        "id": "323611eb-b7ce-4a51-86a2-12b98527d8ae",
        "outputId": "46cc3288-ccdf-4589-9eb7-5f2351a7b131",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 0 length: 113\n",
            "Split 1 length: 298\n",
            "Split 2 length: 234\n",
            "Split 3 length: 140\n",
            "Split 4 length: 8\n",
            "Split 5 length: 141\n",
            "Split 6 length: 199\n",
            "Split 7 length: 6\n",
            "Split 8 length: 142\n",
            "Split 9 length: 193\n",
            "Split 10 length: 262\n",
            "Split 11 length: 204\n",
            "Split 12 length: 298\n",
            "Split 13 length: 299\n",
            "Split 14 length: 295\n",
            "Split 15 length: 295\n",
            "Split 16 length: 252\n",
            "Split 17 length: 294\n",
            "Split 18 length: 295\n",
            "Split 19 length: 295\n",
            "Split 20 length: 294\n",
            "Split 21 length: 296\n",
            "Split 22 length: 211\n",
            "Split 23 length: 253\n",
            "Split 24 length: 297\n",
            "Split 25 length: 244\n",
            "Split 26 length: 161\n",
            "Split 27 length: 185\n",
            "Split 28 length: 155\n",
            "Split 29 length: 295\n",
            "Split 30 length: 299\n",
            "Split 31 length: 294\n",
            "Split 32 length: 250\n",
            "Split 33 length: 85\n",
            "Split 34 length: 299\n",
            "Split 35 length: 230\n",
            "Split 36 length: 283\n",
            "Split 37 length: 164\n",
            "Split 38 length: 294\n",
            "Split 39 length: 294\n",
            "Split 40 length: 298\n",
            "Split 41 length: 294\n",
            "Split 42 length: 298\n",
            "Split 43 length: 296\n",
            "Split 44 length: 295\n",
            "Split 45 length: 258\n",
            "Split 46 length: 266\n",
            "Split 47 length: 145\n",
            "Split 48 length: 164\n",
            "Split 49 length: 294\n",
            "Split 50 length: 299\n",
            "Split 51 length: 299\n",
            "Split 52 length: 294\n",
            "Split 53 length: 292\n",
            "Split 54 length: 243\n",
            "Split 55 length: 98\n",
            "Split 56 length: 298\n",
            "Split 57 length: 299\n",
            "Split 58 length: 239\n",
            "Split 59 length: 291\n",
            "Split 60 length: 290\n",
            "Split 61 length: 297\n",
            "Split 62 length: 299\n",
            "Split 63 length: 254\n",
            "Split 64 length: 282\n",
            "Split 65 length: 209\n",
            "Split 66 length: 170\n",
            "Split 67 length: 297\n",
            "Split 68 length: 245\n",
            "Split 69 length: 286\n",
            "Split 70 length: 205\n",
            "Split 71 length: 222\n",
            "Split 72 length: 175\n",
            "Split 73 length: 87\n",
            "Split 74 length: 249\n",
            "Split 75 length: 258\n",
            "Split 76 length: 36\n",
            "Split 77 length: 297\n",
            "Split 78 length: 295\n",
            "Split 79 length: 250\n",
            "Split 80 length: 56\n",
            "Split 81 length: 221\n",
            "Split 82 length: 299\n",
            "Split 83 length: 297\n",
            "Split 84 length: 292\n",
            "Split 85 length: 243\n",
            "Split 86 length: 298\n",
            "Split 87 length: 299\n",
            "Split 88 length: 294\n",
            "Split 89 length: 299\n",
            "Split 90 length: 297\n",
            "Split 91 length: 267\n",
            "Split 92 length: 295\n",
            "Split 93 length: 296\n",
            "Split 94 length: 278\n",
            "Split 95 length: 292\n",
            "Split 96 length: 240\n",
            "Split 97 length: 199\n",
            "Split 98 length: 296\n",
            "Split 99 length: 221\n",
            "Split 100 length: 293\n",
            "Split 101 length: 297\n",
            "Split 102 length: 233\n",
            "Split 103 length: 299\n",
            "Split 104 length: 299\n",
            "Split 105 length: 274\n",
            "Split 106 length: 290\n",
            "Split 107 length: 244\n",
            "Split 108 length: 250\n",
            "Split 109 length: 225\n",
            "Split 110 length: 113\n",
            "Split 111 length: 295\n",
            "Split 112 length: 12\n",
            "Split 113 length: 298\n",
            "Split 114 length: 294\n",
            "Split 115 length: 299\n",
            "Split 116 length: 297\n",
            "Split 117 length: 299\n",
            "Split 118 length: 287\n",
            "Split 119 length: 248\n",
            "Split 120 length: 264\n",
            "Split 121 length: 296\n",
            "Split 122 length: 296\n",
            "Split 123 length: 247\n",
            "Split 124 length: 93\n",
            "Split 125 length: 296\n",
            "Split 126 length: 296\n",
            "Split 127 length: 296\n",
            "Split 128 length: 247\n",
            "Split 129 length: 101\n",
            "Split 130 length: 298\n",
            "Split 131 length: 251\n",
            "Split 132 length: 297\n",
            "Split 133 length: 293\n",
            "Split 134 length: 296\n",
            "Split 135 length: 299\n",
            "Split 136 length: 209\n",
            "Split 137 length: 242\n",
            "Split 138 length: 295\n",
            "Split 139 length: 75\n",
            "Split 140 length: 188\n",
            "Split 141 length: 183\n",
            "Split 142 length: 229\n",
            "Split 143 length: 41\n",
            "Split 144 length: 298\n",
            "Split 145 length: 252\n",
            "Split 146 length: 122\n",
            "Split 147 length: 200\n",
            "Split 148 length: 297\n",
            "Split 149 length: 298\n",
            "Split 150 length: 299\n",
            "Split 151 length: 226\n",
            "Split 152 length: 295\n",
            "Split 153 length: 211\n",
            "Split 154 length: 119\n",
            "Split 155 length: 195\n",
            "Split 156 length: 299\n",
            "Split 157 length: 298\n",
            "Split 158 length: 271\n",
            "Split 159 length: 29\n",
            "Split 160 length: 299\n",
            "Split 161 length: 193\n",
            "Split 162 length: 287\n",
            "Split 163 length: 265\n",
            "Split 164 length: 182\n",
            "Split 165 length: 234\n",
            "Split 166 length: 268\n",
            "Split 167 length: 298\n",
            "Split 168 length: 185\n",
            "Split 169 length: 76\n",
            "Split 170 length: 264\n",
            "Split 171 length: 298\n",
            "Split 172 length: 133\n",
            "Split 173 length: 231\n",
            "Split 174 length: 84\n",
            "Split 175 length: 279\n",
            "Split 176 length: 236\n",
            "Split 177 length: 175\n",
            "Split 178 length: 280\n",
            "Split 179 length: 291\n",
            "Split 180 length: 284\n",
            "Split 181 length: 285\n",
            "Split 182 length: 259\n",
            "Split 183 length: 250\n",
            "Split 184 length: 190\n",
            "Split 185 length: 249\n",
            "Split 186 length: 283\n",
            "Split 187 length: 289\n",
            "Split 188 length: 298\n",
            "Split 189 length: 175\n",
            "Split 190 length: 271\n",
            "Split 191 length: 247\n",
            "Split 192 length: 282\n",
            "Split 193 length: 286\n",
            "Split 194 length: 273\n",
            "Split 195 length: 250\n",
            "Split 196 length: 194\n",
            "Split 197 length: 287\n",
            "Split 198 length: 186\n",
            "Split 199 length: 289\n",
            "Split 200 length: 298\n",
            "Split 201 length: 251\n",
            "Split 202 length: 26\n",
            "Split 203 length: 293\n",
            "Split 204 length: 296\n",
            "Split 205 length: 299\n",
            "Split 206 length: 225\n",
            "Split 207 length: 219\n",
            "Split 208 length: 278\n",
            "Split 209 length: 245\n",
            "Split 210 length: 249\n",
            "Split 211 length: 176\n",
            "Split 212 length: 299\n",
            "Split 213 length: 246\n",
            "Split 214 length: 280\n",
            "Split 215 length: 271\n",
            "Split 216 length: 272\n",
            "Split 217 length: 276\n",
            "Split 218 length: 279\n",
            "Split 219 length: 263\n",
            "Split 220 length: 240\n",
            "Split 221 length: 299\n",
            "Split 222 length: 275\n",
            "Split 223 length: 210\n",
            "Split 224 length: 281\n",
            "Split 225 length: 226\n",
            "Split 226 length: 18\n",
            "Split 227 length: 48\n",
            "Split 228 length: 289\n",
            "Split 229 length: 299\n",
            "Split 230 length: 298\n",
            "Split 231 length: 298\n",
            "Split 232 length: 285\n",
            "Split 233 length: 294\n",
            "Split 234 length: 297\n",
            "Split 235 length: 297\n",
            "Split 236 length: 295\n",
            "Split 237 length: 298\n",
            "Split 238 length: 298\n",
            "Split 239 length: 296\n",
            "Split 240 length: 288\n",
            "Split 241 length: 292\n",
            "Split 242 length: 298\n",
            "Split 243 length: 296\n",
            "Split 244 length: 295\n",
            "Split 245 length: 298\n",
            "Split 246 length: 298\n",
            "Split 247 length: 149\n",
            "Split 248 length: 289\n",
            "Split 249 length: 282\n",
            "Split 250 length: 292\n",
            "Split 251 length: 245\n",
            "Split 252 length: 26\n",
            "Split 253 length: 295\n",
            "Split 254 length: 296\n",
            "Split 255 length: 298\n",
            "Split 256 length: 296\n",
            "Split 257 length: 292\n",
            "Split 258 length: 295\n",
            "Split 259 length: 297\n",
            "Split 260 length: 295\n",
            "Split 261 length: 299\n",
            "Split 262 length: 281\n",
            "Split 263 length: 132\n",
            "Split 264 length: 295\n",
            "Split 265 length: 297\n",
            "Split 266 length: 295\n",
            "Split 267 length: 207\n",
            "Split 268 length: 293\n",
            "Split 269 length: 299\n",
            "Split 270 length: 294\n",
            "Split 271 length: 219\n",
            "Split 272 length: 142\n",
            "Split 273 length: 255\n",
            "Split 274 length: 282\n",
            "Split 275 length: 243\n",
            "Split 276 length: 242\n",
            "Split 277 length: 244\n",
            "Split 278 length: 260\n",
            "Split 279 length: 245\n",
            "Split 280 length: 290\n",
            "Split 281 length: 225\n",
            "Split 282 length: 293\n",
            "Split 283 length: 183\n",
            "Split 284 length: 245\n",
            "Split 285 length: 290\n",
            "Split 286 length: 298\n",
            "Split 287 length: 255\n",
            "Split 288 length: 262\n",
            "Split 289 length: 268\n",
            "Split 290 length: 254\n"
          ]
        }
      ],
      "source": [
        "for idx, split in enumerate(all_splits):\n",
        "    print(f\"Split {idx} length: {len(split.page_content)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af1ee6fb-70f6-4ce7-8c4d-308a5bbeafc2",
      "metadata": {
        "id": "af1ee6fb-70f6-4ce7-8c4d-308a5bbeafc2"
      },
      "source": [
        "Based on what we read, we'd expect to see some overlap between the end of one split and the beginning of the next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "54720d22-28ec-4ccb-981d-1c0f4f0bbe97",
      "metadata": {
        "id": "54720d22-28ec-4ccb-981d-1c0f4f0bbe97",
        "outputId": "a963aa13-09fd-4546-e2dd-464fb3434409",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "previous: \n",
            "  of $\\angle q, \\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points. \n",
            "\n",
            "current: \n",
            " Fig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\n",
            "Chec \n",
            "\n",
            "\n",
            "------------------\n",
            "\n",
            "previous: \n",
            " ognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities. \n",
            "\n",
            "current: \n",
            " Fig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water.  \n",
            "\n",
            "\n",
            "------------------\n",
            "\n",
            "previous: \n",
            " ral (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API). \n",
            "\n",
            "current: \n",
            " They did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. The \n",
            "\n",
            "\n",
            "------------------\n",
            "\n",
            "previous: \n",
            " the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering. \n",
            "\n",
            "current: \n",
            " ChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use c \n",
            "\n",
            "\n",
            "------------------\n",
            "\n",
            "previous: \n",
            " tform according to the model descriptions and summarize the response based on the execution results. \n",
            "\n",
            "current: \n",
            " Fig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\n",
            "The system comprises \n",
            "\n",
            "\n",
            "------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for prev, curr in zip(all_splits[20:25], all_splits[21:26]):\n",
        "    print('previous: \\n', prev.page_content[-100:], '\\n')\n",
        "    print('current: \\n', curr.page_content[:100], '\\n')\n",
        "    print('\\n------------------\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c392ee3-77d3-4c6f-a76a-bcf0bfb9a8bf",
      "metadata": {
        "id": "0c392ee3-77d3-4c6f-a76a-bcf0bfb9a8bf"
      },
      "source": [
        "But in this slice of splits, I don't see any overlap. (I tried a few different slices and likewise didn't see any overlaps.) Does that mean it's not working?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "4a90f378-4273-4527-bde4-ced79b586318",
      "metadata": {
        "id": "4a90f378-4273-4527-bde4-ced79b586318",
        "outputId": "6870f156-c197-4368-cb46-77f7f6da73e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-------- index 0 ----------\n",
            "\n",
            "previous: \n",
            " Memory \n",
            "\n",
            "current: \n",
            " Memory \n",
            "\n",
            "\n",
            "-------- index 39 ----------\n",
            "\n",
            "previous: \n",
            " GOALS: \n",
            "\n",
            "current: \n",
            " GOALS: \n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range (len(all_splits) - 1):\n",
        "    last = all_splits[i].page_content.strip().split()[-1]\n",
        "    first = all_splits[i+1].page_content.strip().split()[0]\n",
        "    if last == first:\n",
        "        print('\\n-------- index', i, '----------\\n')\n",
        "        print('previous: \\n', last, '\\n')\n",
        "        print('current: \\n', first, '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d93ae62-278a-4ead-9089-49aae6098663",
      "metadata": {
        "id": "4d93ae62-278a-4ead-9089-49aae6098663"
      },
      "source": [
        "There are only two cases where one split overlaps with the previous, and in both cases it looks like a heading. It's not perfectly clear -- at least not to me -- why `RecursiveCharacterTextSplitter` works this way. It could be the nature of the web page (lots of headings, lots of figures). It could be our the relation between our `chunk_size` and `chunk_overlap`. The documentation isn't super helpful. If we want to know more, we'll likely have to dive into the code and experiment.\n",
        "\n",
        "When it comes time to write code for KnotebookLM, we'll likely want to play around with chunk and overlap sizes and see what makes most sense for our application."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20980be4-adaa-4630-860f-b69dcd236804",
      "metadata": {
        "id": "20980be4-adaa-4630-860f-b69dcd236804"
      },
      "source": [
        "### Index Splits\n",
        "\n",
        "If you were implementing this next step without LangChain, you'd likely think of it as two steps:\n",
        "1. For each split, generate an embedding (a vector that represents the \"meaning\" of the text in the split)\n",
        "2. Write the resulting vector and the original text to a database.\n",
        "\n",
        "LangChain handles both with a single call to the `add_documents` method on the `vector_store` instance we created. (And now you understand why we needed to pass the `embeddings` instance as an argument to `vector_store`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "387b14fc-1154-4690-8548-62aaf61e06ad",
      "metadata": {
        "id": "387b14fc-1154-4690-8548-62aaf61e06ad"
      },
      "outputs": [],
      "source": [
        "_ = vector_store.add_documents(documents=all_splits)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f19a7394-3cd4-4cb4-89b5-1db342868b17",
      "metadata": {
        "id": "f19a7394-3cd4-4cb4-89b5-1db342868b17"
      },
      "source": [
        "Let's take a quick peak at what's in `vector_store`. Each document has a unique identifier (the `id`), the `text` and `metadata` we saw when exploring the splits, and a `vector` -- the document's embedding. Maybe it's a little hard to tell, but those embeddings were returned by making calls to the Google embeddings model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "2830e5c4-f05d-4646-a0ab-ad106d07aac4",
      "metadata": {
        "id": "2830e5c4-f05d-4646-a0ab-ad106d07aac4",
        "outputId": "bf64a5ed-5c81-4081-b195-b9938bf78405",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "baf57ac1-16e2-46ef-9c2e-e61085abbdc7: LLM Powered Autonomous Agents\n",
            "    \n",
            "Date: June 23, 2023  |  Estimated Readin\n",
            "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
            "[-0.00840548425912857, 0.001964213792234659, 0.005555008538067341, -0.004316671751439571, -0.004081736318767071]\n",
            "\n",
            "\n",
            "---------------\n",
            "\n",
            "\n",
            "cc30e7c8-e052-4215-8546-8466a3392a4a: Memory\n",
            "\n",
            "Short-term memory: I would consider all the in-context learning (Se\n",
            "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
            "[-0.019172655418515205, 0.008729427121579647, -0.04123864322900772, 0.004078803583979607, -0.00960522424429655]\n",
            "\n",
            "\n",
            "---------------\n",
            "\n",
            "\n",
            "e3211bbb-aa57-42dc-9ec1-3abaea930d94: Fig. 1. Overview of a LLM-powered autonomous agent system.\n",
            "Component One: P\n",
            "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
            "[-0.02587970532476902, 0.03571058064699173, -0.008057745173573494, -0.025214597582817078, 0.022080427035689354]\n",
            "\n",
            "\n",
            "---------------\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for index, (id, doc) in enumerate(vector_store.store.items()):\n",
        "    if index < 3:\n",
        "        # docs have keys 'id', 'vector', 'text', 'metadata'\n",
        "        print(f\"{id}: {doc['text'][:75]}\")\n",
        "        print(doc['metadata'])\n",
        "        print(doc['vector'][:5])\n",
        "        print(\"\\n\\n---------------\\n\\n\")\n",
        "    else:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68d2d870-7e4c-4763-9872-bc5da8ddb48b",
      "metadata": {
        "id": "68d2d870-7e4c-4763-9872-bc5da8ddb48b"
      },
      "source": [
        "That's all the pre-processing we need. We're ready to move on to retrieval tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc8d2856-3dac-40ee-8cd3-3c836d0b72d4",
      "metadata": {
        "id": "bc8d2856-3dac-40ee-8cd3-3c836d0b72d4"
      },
      "source": [
        "## Retrieve Relevant Chunks, Ask Questions\n",
        "\n",
        "We've indexed the web page and can now ask questions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "749c9354-9ca6-4d99-b033-2cbcf6ecec2c",
      "metadata": {
        "id": "749c9354-9ca6-4d99-b033-2cbcf6ecec2c"
      },
      "source": [
        "### Prompt\n",
        "\n",
        "LangChain has a library of task-specific prompts. Let's grab the \"RAG\" prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "3f690343-da3f-41f7-9d22-484517157900",
      "metadata": {
        "id": "3f690343-da3f-41f7-9d22-484517157900",
        "outputId": "1852a89a-5e87-4931-f979-30bd24446ab3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "from langchain import hub\n",
        "\n",
        "prompt = hub.pull('rlm/rag-prompt');\n",
        "prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "379f61ce-7b1a-4f89-aba0-3435885a6dc7",
      "metadata": {
        "id": "379f61ce-7b1a-4f89-aba0-3435885a6dc7"
      },
      "source": [
        "Notice it didn't return a simple string, but rather an instance of `ChatPromptTemplate`. Following the tutorial's walk-through, we can explore it a bit:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "71d03ce4-73dc-4cc1-9c21-ffb132c82808",
      "metadata": {
        "id": "71d03ce4-73dc-4cc1-9c21-ffb132c82808"
      },
      "outputs": [],
      "source": [
        "example_message, = prompt.invoke(\n",
        "    { \"context\": \"Here's where we'll put relevant chunks from the web page.\", \"question\": \"Here's where our question goes.\" }\n",
        ").to_messages()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b550908-7101-444b-96b0-81dcb651898e",
      "metadata": {
        "id": "0b550908-7101-444b-96b0-81dcb651898e"
      },
      "source": [
        "Notice the comma after `example_message`? That wasn't a mistake. As `to_messages` implies, we might get more than one message. Adding the comma there *destructures* the list `to_messages` returns so that I get just the first item. (In this case, there is only one item.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af02ae57-0e11-4f70-af1e-619dc719de6c",
      "metadata": {
        "id": "af02ae57-0e11-4f70-af1e-619dc719de6c"
      },
      "source": [
        "Let's see the `content` of that message..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "2d120448-9c8e-428a-bf5e-e2d69480d804",
      "metadata": {
        "id": "2d120448-9c8e-428a-bf5e-e2d69480d804",
        "outputId": "df95a26a-0171-462d-cd46-1695f401459a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
            "Question: Here's where our question goes. \n",
            "Context: Here's where we'll put relevant chunks from the web page. \n",
            "Answer:\n"
          ]
        }
      ],
      "source": [
        "print(example_message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f404ced-ca53-495c-98f9-108350a02193",
      "metadata": {
        "id": "0f404ced-ca53-495c-98f9-108350a02193"
      },
      "source": [
        "Pretty cool. We pass the prompt a dictionary with `context` and `question` keys and it'll insert their values into our prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06478a0c-43bb-4ae0-9414-4ca90c720279",
      "metadata": {
        "id": "06478a0c-43bb-4ae0-9414-4ca90c720279"
      },
      "source": [
        "### Using LangGraph to Stitch Together the Parts\n",
        "Here's how the docs describe LangGraph:\n",
        "> LangGraph is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows.\n",
        "\n",
        "Translation: LangGraph is a tool for building an application that can coordinate interactions between a user and one or more AI tools -- in this case, our chat and embedding models and the vector store where we stash our document's embeddings. To do so, it needs to \"remember\" (that's what \"stateful\" means).\n",
        "\n",
        "Basically, we'll create the structure of its memory (the `State` class) and define its two operations, `retrieve` and `generate`. Then we define a workflow and \"compile\" it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8588026-ad97-4e97-bfe1-321177cdcee7",
      "metadata": {
        "id": "a8588026-ad97-4e97-bfe1-321177cdcee7"
      },
      "source": [
        "We'll start by importing some classes and types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "94d5dd31-b6c7-474e-af49-f1888f412aae",
      "metadata": {
        "id": "94d5dd31-b6c7-474e-af49-f1888f412aae"
      },
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "from langgraph.graph import START, StateGraph\n",
        "from typing_extensions import List, TypedDict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f19577b4-0fd8-4c07-bc1f-232308b5e1eb",
      "metadata": {
        "id": "f19577b4-0fd8-4c07-bc1f-232308b5e1eb"
      },
      "source": [
        "#### State\n",
        "Next, we'll create the application `State`. It's a class that will inherit from the `TypedDict` class.\n",
        "\n",
        "We need to remember the question, the context (the relevant chunks of the web page), and the answer. The *types* (e.g., `str`, `List`) may look new. We're defining what kind of data will be stored at each property. Type informaton helps us (and our tools) catch errors and can make tools like autocompletion more effective. (Not Python, but some languages are *typed* -- they **require** type information.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "ae5edb68-0345-48f0-a329-f3b5b3c6b3a5",
      "metadata": {
        "id": "ae5edb68-0345-48f0-a329-f3b5b3c6b3a5"
      },
      "outputs": [],
      "source": [
        "class State(TypedDict):\n",
        "    question: str\n",
        "    context: List[Document]\n",
        "    answer: str"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27efc3d2-a99f-434d-a716-7c3b6c321241",
      "metadata": {
        "id": "27efc3d2-a99f-434d-a716-7c3b6c321241"
      },
      "source": [
        "#### Retrieval Task\n",
        "Let's now define the `retrieve` task. This is how we get the most relevant chunks from our document. All we have to do is pass our question to the `similarity_search` method exposed by our `vector_store` instance. The rest is abstracted away. `similarity_search` handles:\n",
        "  1. generating embeddings for our question\n",
        "  2. using those embeddings, searching through the vector store for the chunks closest to it in \"meaning space\"\n",
        "  3. return to us the most similar chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "f37c5c78-dc5a-42be-a8b0-21ba43eef150",
      "metadata": {
        "id": "f37c5c78-dc5a-42be-a8b0-21ba43eef150"
      },
      "outputs": [],
      "source": [
        "def retrieve(state: State):\n",
        "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
        "    return {\"context\": retrieved_docs}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad4cc228-f08b-454a-b546-2e1b6986303a",
      "metadata": {
        "id": "ad4cc228-f08b-454a-b546-2e1b6986303a"
      },
      "source": [
        "We don't have to wait to put together the whole \"graph\" (application). We can test drive `retrieve` ourselves. (Note: to help us later, I'm going to add the result of `retrieve` to `test_state`.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "62cf41cd-6e4d-41c5-bb41-dad7472abcff",
      "metadata": {
        "id": "62cf41cd-6e4d-41c5-bb41-dad7472abcff",
        "outputId": "9ecda777-1b05-4d4b-8dae-ab2762abef2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'Who wrote the blog post?',\n",
              " 'context': [Document(id='a35348bd-ed5f-4959-88ea-dc01414d373a', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='\"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease'),\n",
              "  Document(id='de8a24ad-13dd-45ba-bd44-1130c53a1255', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='\"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease'),\n",
              "  Document(id='11a80e0d-e7fa-4bbf-a9c4-9a2426911147', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Or\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389'),\n",
              "  Document(id='5be9333e-0d89-4126-816a-aafaf955a6c0', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Or\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389')]}"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "test_state = State({ \"question\": \"Who wrote the blog post?\" })\n",
        "test_state[\"context\"] = retrieve(test_state)[\"context\"]\n",
        "test_state"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1212d004-3ad9-4137-a484-6d42dcde2477",
      "metadata": {
        "id": "1212d004-3ad9-4137-a484-6d42dcde2477"
      },
      "source": [
        "I got back four chunks, probably ordered by similarity. None of them seem to contain the relevant information. (The second chunk mentions an author -- and it happens to be the author of the blog post -- but here it's from a citation of another paper.) The author is listed, but our similarity search didn't find it. So I don't have a lot of hope that it'll answer our question, but it will nevertheless be instructive to see how it goes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b9c7766-caf6-458f-929c-5a12a4094d97",
      "metadata": {
        "id": "2b9c7766-caf6-458f-929c-5a12a4094d97"
      },
      "source": [
        "Just to confirm, here's the split that includes the author's name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "5ffdfaec-715f-46f4-abd3-126e393d7093",
      "metadata": {
        "id": "5ffdfaec-715f-46f4-abd3-126e393d7093",
        "outputId": "5cd25901-d688-4c65-a1d0-c0f7dd30d616",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "all_splits[0].page_content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9db9a8ca-2fc5-4729-a5fa-f0e2609ebc80",
      "metadata": {
        "id": "9db9a8ca-2fc5-4729-a5fa-f0e2609ebc80"
      },
      "source": [
        "#### Generation (Question Answering) Task\n",
        "We also need to define a function to construct our prompt, send it to the chat model, and handle its response:\n",
        "  1. Knowing from our exploration that the documents returned from the `retrieve` task are objects with different properties, we first need to extract the `page_content` from each. We'll join them into a single string with a couple of line breaks (`\\n\\n`) separating them.\n",
        "  2. Next, we can use the `prompt` template we created earlier, passing to it the question and the chunks we retrieved and processed.\n",
        "  3. Then we `invoke` our chat model, passing to it the `message` (which we recall is a prompt that contains a kind of system message, the source texts, and our question).\n",
        "  4. The `response` we get back from the chat model has a `content` property. That's what we'll return as the `answer`. (If you're curious, you can print out the full response to see what other details you get.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "4c09ca69-9412-489e-ba25-27da02dbe4c3",
      "metadata": {
        "id": "4c09ca69-9412-489e-ba25-27da02dbe4c3"
      },
      "outputs": [],
      "source": [
        "def generate(state: State):\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
        "    response = llm.invoke(messages)\n",
        "    print(response)\n",
        "    return {\"answer\": response.content}"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VS1n1NJbGsx2"
      },
      "id": "VS1n1NJbGsx2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "94fe1f81-125e-4d8d-967b-8be4fb44c28c",
      "metadata": {
        "id": "94fe1f81-125e-4d8d-967b-8be4fb44c28c"
      },
      "source": [
        "Let's give it a try using our example question and the documents we retrieved based on that question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "3baddcba-f263-45e8-8f34-278deaa232e2",
      "metadata": {
        "id": "3baddcba-f263-45e8-8f34-278deaa232e2",
        "outputId": "edfc2efc-278f-4b51-9b51-81a63a89fb87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Lilian Weng wrote the blog post \"LLM-powered Autonomous Agents\". The blog post was published on lilianweng.github.io in June 2023. The URL for the blog post is https://lilianweng.github.io/posts/2023-06-23-agent/.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-69786c16-eb57-469f-8a08-be2ba4fbd902-0' usage_metadata={'input_tokens': 1269, 'output_tokens': 72, 'total_tokens': 1341, 'input_token_details': {'cache_read': 0}}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'Lilian Weng wrote the blog post \"LLM-powered Autonomous Agents\". The blog post was published on lilianweng.github.io in June 2023. The URL for the blog post is https://lilianweng.github.io/posts/2023-06-23-agent/.'}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "generate(test_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e51e088c-f886-4092-9243-71765a6f3045",
      "metadata": {
        "id": "e51e088c-f886-4092-9243-71765a6f3045"
      },
      "source": [
        "As an experiment, let's see what happens if I add what I know is the relevant information to the context."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_state[\"context\"])"
      ],
      "metadata": {
        "id": "9nUukItUGuGq",
        "outputId": "8f8cb254-49c5-4de4-b857-6bf0c768660b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "9nUukItUGuGq",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(id='a35348bd-ed5f-4959-88ea-dc01414d373a', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='\"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease'), Document(id='de8a24ad-13dd-45ba-bd44-1130c53a1255', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='\"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease'), Document(id='11a80e0d-e7fa-4bbf-a9c4-9a2426911147', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Or\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389'), Document(id='5be9333e-0d89-4126-816a-aafaf955a6c0', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Or\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "c13cb102-bd0f-4e97-9a31-859c6e17e643",
      "metadata": {
        "id": "c13cb102-bd0f-4e97-9a31-859c6e17e643",
        "outputId": "b4c431e9-f222-4db9-855c-d95691854895",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='The blog post \"LLM-powered Autonomous Agents\" was written by Lilian Weng. It was published on June 23, 2023. The blog post discusses building agents with LLM as the core controller.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-13a5bc44-2fe2-459f-8e13-deb125999d4e-0' usage_metadata={'input_tokens': 1271, 'output_tokens': 48, 'total_tokens': 1319, 'input_token_details': {'cache_read': 0}}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'The blog post \"LLM-powered Autonomous Agents\" was written by Lilian Weng. It was published on June 23, 2023. The blog post discusses building agents with LLM as the core controller.'}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "test_state[\"context\"].append(all_splits[0])\n",
        "generate(test_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ac26d7f-62bf-4977-8032-f7814a6b603b",
      "metadata": {
        "id": "3ac26d7f-62bf-4977-8032-f7814a6b603b"
      },
      "source": [
        "Sweet! That time, we got it. It would be worth experimenting to see if we can tweak our parameters or text-splitting strategy to improve our results. The tool is no good if we have to find the answer and pass it to the LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96c6e97b-68c5-4802-9c9a-b4779f5126df",
      "metadata": {
        "id": "96c6e97b-68c5-4802-9c9a-b4779f5126df"
      },
      "source": [
        "#### String the Tasks Together\n",
        "Finally, let's add our task definitions to an instance of `LangGraph` and \"compile\" it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "e01108bd-e836-45f9-92c6-2e4e51f6232b",
      "metadata": {
        "id": "e01108bd-e836-45f9-92c6-2e4e51f6232b"
      },
      "outputs": [],
      "source": [
        "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, \"retrieve\")\n",
        "graph = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05038419-c953-4020-8ada-20d92ee20d8d",
      "metadata": {
        "id": "05038419-c953-4020-8ada-20d92ee20d8d"
      },
      "source": [
        "[link text](https:// [link text](https://))Now, instead of running each task ourselves, we should be able to `invoke` the graph with a question and get an answer. Let's try it with a new question:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "4c0a05f5-5f45-4857-8398-5c7c038c9a60",
      "metadata": {
        "id": "4c0a05f5-5f45-4857-8398-5c7c038c9a60",
        "outputId": "d5302a95-e1d1-4bab-8f92-c9dfcdbab870",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Task decomposition is a technique that transforms big tasks into multiple manageable tasks. This is done by instructing the model to \"think step by step.\"  It can be done by a LLM, using task-specific instructions, or with human input.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-ab9845de-b32e-4582-af4e-4abb69407fbf-0' usage_metadata={'input_tokens': 632, 'output_tokens': 50, 'total_tokens': 682, 'input_token_details': {'cache_read': 0}}\n",
            "Task decomposition is a technique that transforms big tasks into multiple manageable tasks. This is done by instructing the model to \"think step by step.\"  It can be done by a LLM, using task-specific instructions, or with human input.\n"
          ]
        }
      ],
      "source": [
        "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
        "print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "179f864b-289e-4fe3-8d83-31620aa2df50",
      "metadata": {
        "id": "179f864b-289e-4fe3-8d83-31620aa2df50",
        "outputId": "ca84968e-8765-47e1-a831-4f023cd30e54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Task decomposition transforms big tasks into multiple manageable tasks. These tasks are smaller and simpler steps. This is achieved through techniques like chain of thought.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-4f8f8508-3974-442e-a92d-75bf6012d927-0' usage_metadata={'input_tokens': 680, 'output_tokens': 29, 'total_tokens': 709, 'input_token_details': {'cache_read': 0}}\n",
            "Task decomposition transforms big tasks into multiple manageable tasks. These tasks are smaller and simpler steps. This is achieved through techniques like chain of thought.\n"
          ]
        }
      ],
      "source": [
        "response2 = graph.invoke({\"question\": \"You said decomposition breaks large tasks into manageable ones. What makes a task more manageable?\" })\n",
        "print(response2[\"answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "037db24d-221f-4361-801b-23df24872f8d",
      "metadata": {
        "id": "037db24d-221f-4361-801b-23df24872f8d"
      },
      "source": [
        "## Evaluation, Next Steps\n",
        "What do you think of LangChain? Hard to say since you don't have much experience with it and (probably) even less with other libraries that try to do the same. That's the position you often find yourself in: do I commit to spending more time learning this library? Move on to another??\n",
        "\n",
        "We can also ask how we'd evaluate our application. A couple of things to consider:\n",
        "  - The application has many parts. How can we judge (and possibly improve) each separately? Once we've chained everything together with `LangGraph`, it's really hard to tell where things are failing or not working well. For example, if we'd just asked about the author without looking separately at the retrieval and generation tasks, we wouldn't know that it fails because we just don't retrieve the right information.\n",
        "     - We also have to consider that later steps depend on earlier steps. For example, if retrieval isn't working, maybe we need to tweak how we're chunking the text.\n",
        "  - we relied on a particular embedding and chat model. Would others do better? Worse? What counts as better?\n",
        "\n",
        "We should also take stock of what this application **doesn't** do. For example, each question we ask will be isolated. `reponse2` shows that we haven't built an application capable of holding a conversation where previous conversational turns influence the next response."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}